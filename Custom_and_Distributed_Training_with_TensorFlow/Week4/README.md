# Distributed Training

This week, you will harness the power of distributed training to process more data and train larger models, faster. You’ll get an overview of various distributed training strategies and then practice working with two strategies, one that trains on multiple GPU cores, and the other that trains on multiple TPU cores. Get your cape ready, because you’re going to get some superpowers this week!

## Learning Objectives

- Explain how distributed training is different from regular model training
- Use the Mirrored Strategy to train a model on multiple GPUs on the same device
- Use the TPU Strategy to train on multiple cores of a TPU

## Overview of Distribution Strategies

- [Video - Intro to distribution strategies](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/ARLeQ/intro-to-distribution-strategies)

- [Video - Types of distribution strategies](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/oe6cz/types-of-distribution-strategies)

## Mirrored Strategy

- [Video - Converting code to the Mirrored Strategy](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/k7ml3/converting-code-to-the-mirrored-strategy)

- [Video - Mirrored Strategy code walkthrough](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/nQ7DB/mirrored-strategy-code-walkthrough)

- [Lab - Mirrored Strategy](./Labs/C2_W4_Lab_1_basic-mirrored-strategy.ipynb)

## Multiple GPU Mirrored Strategy

- [Video - Custom Training for Multiple GPU Mirrored Strategy](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/EDiRd/custom-training-for-multiple-gpu-mirrored-strategy)

- [Video - Multi GPU Mirrored Strategy code walkthrough](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/21zgD/multi-gpu-mirrored-strategy-code-walkthrough)

- [Lab - Multi GPU Mirrored Strategy](./Labs/C2_W4_Lab_2_multi-GPU-mirrored-strategy.ipynb)

## TPU Strategy

- [Video - TPU Strategy](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/etSkY/tpu-strategy)

- [Video - TPU Strategy code walkthrough](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/8q8PR/tpu-strategy-code-walkthrough)

- [Lab - TPU Strategy](./Labs/C2_W4_Lab_3_using-TPU-strategy.ipynb)

## Other Distributed Strategies

- [Video - Other Distributed Strategies](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/vUZqv/other-distributed-strategies)

- [Reading - References used in Other Distributed Strategies](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)

- [Lab - One Device Strategy](./Labs/C2_W4_Lab_4_one-device-strategy.ipynb)

## Lecture Notes (Optional)

- [Reading - Lecture Notes Week 4](./Readings/C2_W4.pdf)

## Assignment: Distributed Strategy

- [Lab - Distributed Strategy](./Labs/C2W4_Assignment.ipynb)

## Course Resources

- [Reading - References](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/supplement/NMmKG/references)